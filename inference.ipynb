{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "219e119ef218c53e8108e50f0ffb4ba3048be25c93fc046b14d9f630afecca5c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from pyarabic.araby import strip_tashkeel,strip_tatweel,normalize_ligature\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-02 12:42:00.689992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "labels = ['DIAL_EGY', 'DIAL_GLF', 'DIAL_LEV', 'MSA']\n",
    "label2id={label:idx for idx,label in enumerate(labels)}\n",
    "id2label={idx:label for label,idx in label2id.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def preprocess_text(sequence):\n",
    "    outputs=[]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for tweet in sequence:\n",
    "        tweet = str(tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash # sign from the word\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'@[\\w]+','',tweet)\n",
    "        #replace punctuations with space\n",
    "        tweet = re.sub(r\"[,.;@#?!&$_]+\\ *\", \" \", tweet)\n",
    "        #find arabic letters only\n",
    "        tweet = ' '.join(re.findall(r'[\\u0600-\\u06FF]+',tweet))\n",
    "        #remove tashkeel\n",
    "        tweet = strip_tashkeel(tweet)\n",
    "        #remove tatweel\n",
    "        tweet = strip_tatweel(tweet)\n",
    "        #apply normalization\n",
    "        #tweet = normalize_ligature(tweet)\n",
    "        #tokenize tweets\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        #tweet_tokens = tweet.split(' ')\n",
    "        tweet_clean=[]\n",
    "        for word in tweet_tokens: # Go through every word in your tokens list\n",
    "            #if word not in string.punctuation:  # remove punctuation\n",
    "            #    tweet_clean.append(word)\n",
    "            word_reg = re.compile(r'\\w')\n",
    "            if word_reg.search(word):\n",
    "                tweet_clean.append(word)\n",
    "        outputs.append((' '.join(tweet_clean)))\n",
    "    return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "model = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-02 12:42:18.144131: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-02 12:42:18.144398: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/9.2.0/lib:/cm/local/apps/gcc/9.2.0/lib64:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2021-09-02 12:42:18.144412: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-02 12:42:18.144429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (loginnode): /proc/driver/nvidia/version does not exist\n",
      "2021-09-02 12:42:18.144652: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-02 12:42:18.147935: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "Some layers from the model checkpoint at asafaya/bert-base-arabic were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at asafaya/bert-base-arabic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def tokenize_tweets(tweets, tokenizer, max_seq_len = 128):\n",
    "    tokenized_tweets = []\n",
    "    oov_tokens=set()\n",
    "    for tweet in tqdm(tweets):\n",
    "        tokenized_tweet = tokenizer.encode(\n",
    "                            tweet,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                            truncation=True\n",
    "                    )\n",
    "        #oov_tokens.update([w for w in tweet.split(' ') if w not in tokenizer.get_vocab()])\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "    #print('num of oov',len(oov_tokens))\n",
    "    return tokenized_tweets\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_tweets):\n",
    "    attention_masks = []\n",
    "    for tweet in tokenized_and_padded_tweets:\n",
    "        att_mask = [int(token_id > 0) for token_id in tweet]\n",
    "        attention_masks.append(att_mask)\n",
    "    return np.asarray(attention_masks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class BertModel(tf.keras.Model):\n",
    "    def __init__(self,bert,units,rate=0.0):\n",
    "        super(BertModel,self).__init__()\n",
    "        self.bert=bert\n",
    "        self.dropout=tf.keras.layers.Dropout(rate)\n",
    "        self.dense=tf.keras.layers.Dense(4,activation='softmax')\n",
    "\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        x=self.bert(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,position_ids=position_ids,head_mask=head_mask)#(batch,seqlength,emb_dim)\n",
    "        outputs=self.dropout(x[1])\n",
    "        outputs=self.dense(outputs)\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "DI_model=BertModel(model,len(labels),0.5)\n",
    "DI_model.built=True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def inference(model,tokenizer,input_text):\n",
    "    input_text = preprocess_text([input_text])\n",
    "    input_text_tokenized = tokenize_tweets(input_text,tokenizer)\n",
    "    input_text_padded = tf.keras.preprocessing.sequence.pad_sequences(input_text_tokenized,128)\n",
    "    input_text_att_mask = create_attention_masks(input_text_padded)\n",
    "    outputs = model(input_ids=input_text_padded, attention_mask=input_text_att_mask, training=False)[0]\n",
    "    print(outputs.numpy().max())\n",
    "    return (id2label[outputs.numpy().argmax(axis=-1)])\n",
    "    \n",
    "DI_model.load_weights('/home/nessam/Nader/Research/Research/AOC/models/BertBase asafaya/')\n",
    "prediction = inference(DI_model,tokenizer,\"انا بحب  كده\")\n",
    "print(prediction)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 287.60it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9540164\n",
      "DIAL_EGY\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}